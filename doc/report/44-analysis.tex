\subsection{Analysis}
\label{sec:analysis}

As shown by the values in \cref{fig:seq}, the advantage in using simulated annealing is clear. By letting the algorithm follow worse solutions in the beginning, it achieved values around 78\% better (average) than those of the basic version. In fact, in the sequential version, the basic approach never even obtained the perfect solution.

Taking advantage of parallelism, therefore allowing more solutions to be computed at the same time introduced even greater improvements. But even in the largest test (192 processes) the basic approach barely touched the perfect solutions. The simulated annealing approach, on the other hand, obtained the perfect solution in most cases for 16 processes or more. In the largest case, it in fact obtained the perfect solution for each value of $n$ in a test with 10 executions. Compared with the basic approach in the sequential version, the improvement with simulated annealing and using 16 processes is around 94\% (average) -- 100\% using 192 processes.

Changing the initial temperature in the simulated annealing approach did not influence (almost at all) the algorithm's execution for the values selected for $n$. Partial solutions with greater values for $n$ (between 500 and 1000) showed the behavior described in \cite{Quinn2004}. Yet, since no useful complete execution could be retrieved to include in this document, the initial range, retested using a limited number of iterations, showed that greater initial temperatures cause the algorithm to diverge at first, which slows down the convergence of the method.
